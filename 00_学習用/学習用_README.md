
# プロジェクト名: LLM Fine-Tuning Template with Unsloth

このプロジェクトは、Unslothを使用したLLM（大規模言語モデル）の微調整テンプレートです。
再利用性と保守性を重視し、YAML設定ファイルによる柔軟な設定変更を可能にしています。

## ディレクトリ構造
以下はプロジェクトのディレクトリ構造です：

```
project/
├── config.yaml            # 設定ファイル（モデル、トレーニング、データセット設定）
├── main.py                # エントリーポイント、プロジェクト全体の流れを制御
├── dataset.py             # データセット準備用モジュール
├── trainer.py             # トレーナー初期化とトレーニングロジック
├── model_factory.py       # モデル作成ファクトリ（LoRA設定を含む）
└── utils.py               # ユーティリティ関数（拡張可能）
```

## 使用方法
### 必要な準備
1. **依存関係のインストール**:
   - 必要なライブラリをインストールしてください。
     ```bash
     pip install -r requirements.txt
     ```

2. **設定ファイルの確認 (`config.yaml`)**:
   - 必要に応じて、モデルIDやデータセットのパスを変更してください。

3. **データセットの準備**:
   - 指定されたデータセットをダウンロードし、適切なパスに配置してください。
   - 例: `./ichikara-instruction-003-001-1.json`

### 実行手順
1. **main.pyの実行**:
   - 次のコマンドで実行できます：
     ```bash
     python main.py
     ```

2. **モデルのトレーニング結果**:
   - `outputs/`ディレクトリに保存されます。

### トレーニング後のモデル使用
- トレーニング済みのモデルをHugging Face Hubにアップロードする場合、`HF_TOKEN`を設定してください。

## カスタマイズ方法
1. **モデルの変更**:
   - `config.yaml` 内の `model.id` を変更してください。

2. **データセットフォーマットの変更**:
   - `config.yaml` 内の `dataset.prompt_format` をカスタマイズすることで、任意のプロンプト形式に変更可能です。

3. **トレーニングパラメータの調整**:
   - `config.yaml` 内の `trainer` セクションでバッチサイズやエポック数などを調整してください。

## 注意事項
- 本コードはGoogle Colabおよび24GB GPU環境での動作を想定しています。
- ライセンスに準拠し、データセットやモデルの使用時に注意してください。

---

