model:
  base_model_id: "llm-jp/llm-jp-3-13b"
  #base_model_id:  "google/gemma-2-9b"
  new_model_path: "./results"
  new_model_id: "llm-jp-3-13b-finetune"
  max_seq_length: 512
  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    bias: "none"

quantization:
  load_in_4bit: true
  quant_type: "nf4"
  compute_dtype: "bfloat16"

training:
  output_dir: "./results/outputs"
  batch_size: 1
  gradient_accumulation: 4
  learning_rate: 5.0e-5
  num_epochs: 1
  warmup_steps: 10
  save_steps: 100
  logging_steps: 10
  group_by_length: true

dataset:
  #train_file: "./dataset/ichikara-instruction-003-001-1.json"
  #train_file: "./dataset/addThink_ichikara-instruction-003-001-1.json"
  train_file: "./dataset/merged_dataset.json"
  eval_file: "./dataset/elyza-tasks-100-TV_0.jsonl"
  prompt_format: "### 指示\n{}\n### 回答\n{}"

huggingface:
  hf_token: "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"
  private: false

process_config:
  input_filepaths:
  - "./dataset/ichikara-instruction-003-001-1.json"
  - "./dataset/ichikara-instruction-003-001-2.1.json"
  - "./dataset/ichikara-instruction-003-001-2.2.json"
  - "./dataset/ichikara-instruction-003-001-5.1.json"
  - "./dataset/ichikara-instruction-003-001-5.2.json"
  - "./dataset/ichikara-instruction-003-002-1.json"
  # - "./dataset/ichikara-instruction-003-003-1.json" #重複データなのでオミット
  merged_filepath: dataset/merged_dataset.json
  prefixed_filepath: dataset/prefixed_dataset.json
  split_processed_filepath: dataset/split_processed_dataset.json
  prepend_text: "う〜んなるほど…よく考えさせてください。\n"
  split_string: "### 回答\n"

#model:
#  #id: "llm-jp/llm-jp-3-13b"
#  id: "google/gemma-2-9b-it"
#  max_seq_length: 512
#  dtype: null
#  load_in_4bit: false
#  trust_remote_code: false
#  lora:
#    r: 32
#    alpha: 32
#    dropout: 0.05
#    bias: none
#    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
#
#trainer:
#  batch_size: 2
#  gradient_accumulation_steps: 4
#  num_epochs: 1
#  logging_steps: 10
#  save_steps: 100
#  learning_rate: 0.0002
#  #2e-4
#  fp16: false
#  output_dir: "outputs"
#
#dataset:
#  path: "./ichikara-instruction-003-001-1.json"
#  prompt_format: "### 指示\n{}\n### 回答\n{}"

#memo: |-
#    training_arguments: 学習の設定
#      - output_dir:トレーニング後のモデルを保存するディレクトリ
#      - per_device_train_batch_size:
#          - デバイスごとのトレーニングバッチサイズ
#      - per_device_eval_batch_size:
#          - デバイスごとの評価バッチサイズ
#      - gradient_accumulation_steps:
#          - 勾配を更新する前にステップを積み重ねる回数
#      - optim:
#          - オプティマイザの設定
#      - num_train_epochs:
#          - エポック数
#      - eval_strategy:
#          - 評価の戦略 ("no"/"steps"/"epoch")
#      - eval_steps:eval_strategyが"steps"のとき、評価を行うstep間隔
#      - logging_strategy:ログ記録の戦略
#      - logging_steps:ログを出力するステップ間隔
#      - warmup_steps:学習率のウォームアップステップ数
#      - save_steps:モデルを保存するステップ間隔
#      - save_total_limit:保存しておくcheckpointの数
#      - max_steps:トレーニングの最大ステップ数
#      - learning_rate:学習率
#      - fp16:16bit浮動小数点の使用設定（第8回演習を参考にすると良いです）
#      - bf16:BFloat16の使用設定
#      - group_by_length:入力シーケンスの長さによりバッチをグループ化 (トレーニングの効率化)
